<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>CS 180 Final Project - by Eshani Jha</title>
    <style>
        body {
            font-family: 'Cambria', serif; /* Cambria font */
            background-color: #f0f0f0;
            margin: 0;
            padding: 20px;
            font-size: 18px; /* Increased base font size */
        }

        /* Top header with navy blue to rich purple blend (top to bottom) */
        .gradient-header {
            background: linear-gradient(to bottom, #000080, #800080); /* Navy Blue to Rich Purple */
            color: white;
            padding: 15px;
            margin-bottom: 10px;
            border-radius: 10px;
            text-align: center;
        }
        
        /* Section headings with maroon to bright orange gradient (left to right) */
        .section-heading {
            background: linear-gradient(to right, #800000, #FF00FF); /* Magenta to Forest Green */
            color: white;
            padding: 10px;
            margin-bottom: 5px; /* Reduced margin to close gap */
            border-radius: 10px;
        }
        
        /* Subsection headings with magenta to forest green gradient (top to bottom) */
        .subsection-heading {
            background: linear-gradient(to right, #800000, #FFA500); /* Maroon to Bright Orange */
            color: white;
            padding: 8px;
            margin-bottom: 5px; /* Reduced margin */
            border-radius: 10px;
        }


        h1 {
            font-size: 32px; /* Increased font size */
            margin: 0;
        }

        h2 {
            font-size: 26px; /* Increased font size */
            margin-bottom: 5px; /* Reduced margin to close gap */
        }

        h3 {
            font-size: 22px; /* Increased font size */
        }

        p {
            color: #333;
            line-height: 1.7; /* Improved line spacing for readability */
            font-size: 20px; /* Increased paragraph font size */
        }

        img {
            display: block;
            max-width: 100%;
            height: auto;
            margin: 0 auto 10px auto; /* Center images horizontally */
            border: 2px solid #ddd;
            border-radius: 10px;
        }
        
        .content {
            padding: 5px 0; /* Reduced padding */
        }
    </style>
</head>
<body>

    <div class="gradient-header">
        <h1>CS 180 Final Project Part 1 - Light Field Camera</h1>
    </div>

    <div>
        <h2 class="section-heading">Overview</h2>
        <div class="content">
          <p>In this project, I recreated the effects of light field cameras using techniques like depth refocusing and aperture adjustment, inspired by the paper 
            <em>Light Field Photography with a Hand-held Plenoptic Camera</em> by Ren Ng et al. (Ren Ng, founder of the Lytro camera, is also a professor at 
            Berkeley!). By capturing multiple images over a plane orthogonal to the optical axis, complex effects such as dynamic focus and aperture control 
            can be achieved through simple operations like shifting and averaging. </p>
          <p> Using the Stanford Light Field Archive dataset, which contains images captured from a 17x17 
            camera array, I simulated varying focal lengths and aperture sizes. Each image corresponds 
            to a unique camera position, enabling the exploration of these light field techniques with 
            real-world data. </p>
          <p> My goal was to reproduce the fascinating visual effects described in Section 4 of the paper. 
            By leveraging the rectified image datasets provided by the Stanford Light Field Archive, I 
            implemented depth refocusing and aperture adjustment to demonstrate how elementary operations 
            on light field data can produce visually stunning results. </p>
            
        </div>
    </div>

    <div>
        <h2 class="section-heading">Depth Refocusing</h2>
        <div class="content">
            <p> Depth refocusing leverages the principle that objects farther from the camera exhibit 
              minimal positional variation across images in a light field, while closer objects shift 
              significantly when the camera moves. By appropriately shifting and averaging images 
              from a 17x17 camera grid, we can align objects at specific depths. This effectively simulates
              a change in focus. </p>
            <p> In this project, I used the center image at position (8, 8) as my reference and 
              shifted each image in the grid by \(\alpha \cdot (i - 8, j - 8)\), where \(\alpha\) is 
              a variable parameter that controls the focal depth. Averaging the shifted images 
              allows for dynamically focusing on objects at different distances. Without any shifting, 
              the final image appears sharp for far-away objects, but blurry for closer objeccts. </p>
        </div>

      <h3 class="subsection-heading">Results</h3>
        <div class="content">
          <p> This technique produces effects similar to adjusting the depth of focus in the 
            dataset's online viewer under the "Full Aperture" setting. Below are examples demonstrating 
            how changing the parameter \(\alpha\) creates images focused at varying depths.</p>

<div style="text-align: center; margin-bottom: 30px;">
  <!-- Candy Depth GIF -->
  <div style="margin: 20px;">
    <img src="candy_depth_gif.gif" alt="Candy Depth Refocusing" style="max-width: 500px; max-height: 500px;">
    <p>\(\alpha\): -5 to 4 (incremented by 1)</p>
  </div>

  <!-- Chess Depth GIF -->
  <div style="margin: 20px;">
    <img src="chess_depth_gif.gif" alt="Chess Depth Refocusing" style="max-width: 500px; max-height: 500px;">
    <p>\(\alpha\): -3 to 3 (incremented by 1)</p>
  </div>

  <!-- Cards Depth GIF -->
  <div style="margin: 20px;">
    <img src="cards_depth_gif.gif" alt="Cards Depth Refocusing" style="max-width: 500px; max-height: 500px;">
    <p>\(\alpha\): -4 to 9 (incremented by 1)</p>
  </div>
</div>

      
    </div>

    <div>
        <h2 class="section-heading">Aperture Adjustment</h2>
        <div class="content">
            <p>Aperture adjustment simulates the effect of varying aperture sizes in a light field camera 
               by controlling the number of images used in the averaging process. Averaging a large 
              number of images from the camera grid mimics a camera with a large aperture, capturing 
              more light and resulting in a shallower depth of field. Conversely, using fewer 
              images corresponds to a smaller aperture, with a greater depth of field.</p>
          <p> To achieve this, I introduced a hyperparameter \(\beta\) to control the radius of 
            the camera grid. Only images satisfying \(|i - 8| \leq \beta\) and \(|j - 8| \leq \beta\) were 
            included in the shifting and averaging process. By varying \(\beta\), I generated images 
            corresponding to different aperture sizes, all while maintaining focus on the same 
            depth. Larger \(\beta\) values produce a blurred background with focused objects, while 
            smaller \(\beta\) values retain sharpness across the scene. </p>

      <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <p>Below are examples illustrating how changing the parameter \(\beta\) simulates varying aperture sizes and 
                their impact on the depth of field. Increasing \(\beta\) corresponds to using more images in the averaging 
                process, mimicking a larger aperture.
  </p>
  <p>
    The process worked particularly well for the candy and chess datasets, likely due to the presence of distinct depth layers 
      and clear spatial details. The cards dataset, however, performed less effectively. This is because this image data set lacks 
      significant depth variations, making the aperture adjustment less noticeable.
  </p>
        </div>

<div style="text-align: center; margin-bottom: 30px;">
  <!-- Candy Depth GIF -->
  <div style="margin: 20px;">
    <img src="candy_aperture_gif.gif" alt="Candy Depth Refocusing" style="max-width: 500px; max-height: 500px;">
    <p>\(\beta\): 0 to 8 (incremented by 1)</p>
  </div>

  <!-- Chess Depth GIF -->
  <div style="margin: 20px;">
    <img src="chess_aperture_gif.gif" alt="Chess Depth Refocusing" style="max-width: 500px; max-height: 500px;">
    <p>\(\beta\): 0 to 8 (incremented by 1)</p>
  </div>

  <!-- Cards Depth GIF -->
  <div style="margin: 20px;">
    <img src="cards_aperture_gif.gif" alt="Cards Depth Refocusing" style="max-width: 500px; max-height: 500px;">
    <p>\(\beta\): 0 to 8 (incremented by 1)</p>
  </div>
</div>
      
    </div>
          
    </div>

          <div>
        <h2 class="section-heading">Summary</h2>
        <div class="content">
           <p>This project was an exciting exploration of light fields and their applications in 
             computational photography. I learned how simple operations like shifting and 
             averaging across a camera grid can produce complex effects like depth refocusing and 
             aperture adjustment. It was fascinating to see how different aspects of a camera, 
             such as aperture size and focal depth, contribute to the overall visual effects in an image. </p>
        </div>

    <div class="gradient-header">
        <h1>Project Part 2 - Eulerian Video Magnification</h1>
    </div>
      
    <div>
        <h2 class="section-heading">Overview</h2>
        <div class="content">
               <p> Eulerian Video Magnification (Wu et al., 2012) is a method to amplify subtle variations in videos, 
                   such as blood flow and low-amplitude motion. By decomposing video frames into spatial frequency 
                   bands, isolating temporal variations using frequency domain filters, and amplifying these signals, 
                   we can visualize changes that are otherwise imperceptible to the naked eye. In this project, 
                   I implemented Eulerian Video Magnification to analyze three videos: <strong>face.mp4</strong>, 
        <strong>baby2.mp4</strong>, and my custom video <strong>myhand.mp4</strong>. Each video highlights 
        different types of signals, from blood flow to subtle movements.
    </p>

    <div>
        <div class="section">
    <h3>Laplacian Pyramids</h3>
    <p>
        The first step was to construct Laplacian pyramids for each video frame. Each level of the Laplacian pyramid isolates spatial frequency bands. I built the pyramid by:
    </p>
    <ol>
        <li>Creating a Gaussian pyramid, where each successive level is blurred and downsized.</li>
        <li>Computing the difference between adjacent Gaussian levels to construct the Laplacian levels.</li>
        <li>Processing all frames in the video to create Laplacian pyramids for temporal filtering.</li>
    </ol>
    
    <h3>Temporal Filtering</h3>
    <p>
        Temporal filtering isolates the desired frequency bands in the time domain. Using a Butterworth band-pass filter, I filtered pixel values across time to extract subtle variations. For example:
    </p>
    <ul>
        <li><strong>face.mp4:</strong> Frequencies between 0.83–1.0 Hz were targeted to amplify blood flow signals.</li>
        <li><strong>baby2.mp4:</strong> Frequencies between 2.33–2.67 Hz were chosen to highlight pulsations.</li>
        <li><strong>myhand.mp4:</strong> Frequencies between 0.6–1.2 Hz were amplified to reveal my pulse.</li>
    </ul>

    <h3>Signal Amplification</h3>
    <p>
        After filtering, the extracted signals were amplified by a factor of <code>alpha = 10</code>. Equalization constants were applied to balance the amplification:
    </p>
    <ul>
        <li><strong>face.mp4:</strong> Equalization constant = 0.31</li>
        <li><strong>baby2.mp4:</strong> Equalization constant = 0.24</li>
        <li><strong>myhand.mp4:</strong> Equalization constant = 0.24</li>
    </ul>
    <p>
        These amplified signals were added back to the original signals, emphasizing subtle variations.
    </p>

    <h3>Reconstruction</h3>
    <p>
        The final step was reconstructing the video by collapsing the Laplacian pyramid back into full-resolution frames. The magnified signals became visible in the final output videos. The original input videos are diplayed below.
    </p>

            
</div>

        <h2 class="section-heading">Results</h2>
        <div class="content">
    <h3>1. face.mp4</h3>
    <p>
      Frequencies between 0.83–1.0 Hz were amplified to reveal blood flow. The t_start and t_end parameters were set to 85 and 245. Below is the resulting video:
    </p>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/your_face_video_id" frameborder="0" allowfullscreen></iframe>

    <h3>2. baby2.mp4</h3>
    <p>
      Frequencies between 2.33–2.67 Hz were targeted to amplify pulsations. The t_start and t_end parameters were set to 23 and 253. Here is the result:
    </p>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/your_baby2_video_id" frameborder="0" allowfullscreen></iframe>

    <h3>3. myhand.mp4</h3>
    <p>
      For my custom video I captured the video used with my phone, and then used an online video compression tool to shrink the file size. 
        Frequencies between 0.6–1.2 Hz were amplified to highlight my blood flow. The t_start and t_end parameters were also set to 23 and 253:
    </p>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/your_myhand_video_id" frameborder="0" allowfullscreen></iframe>
  </div>

  <div class="section">
<h2 class="section-heading">Bells and Whistles</h2>
        <div class="content">
    <p>
      I explored the impact of modifying the frequency ranges:
    </p>
    <ul>
      <li><strong>Narrowed Range:</strong> For <em>face.mp4</em>, narrowing to 2.0–2.2 Hz highlighted fine details, while for <em>baby2.mp4</em>, narrowing to 0.4–0.7 Hz reduced noise but limited pulsation visibility.</li>
      <li><strong>Widened Range:</strong> Expanding the range to 0.8–2.5 Hz for <em>face.mp4</em> and 0.5–3.0 Hz for <em>baby2.mp4</em> enhanced broader signals but introduced artifacts.</li>
    </ul>
  </div>

<h2 class="section-heading">Challenges</h2>
        <div class="content">
    <p>
      One key challenge was tuning parameters like frequency range and equalization constants to balance magnification 
        and artifact suppression. High magnification often led to noise, while overly narrow frequency ranges limited 
        visible effects. Additionally, the computational cost of processing high-resolution videos required efficient 
        implementation.
    </p>
  </div>
</div>

            
</body>
</html>
