<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>CS 180 Final Project - by Eshani Jha</title>
    <style>
        body {
            font-family: 'Cambria', serif; /* Cambria font */
            background-color: #f0f0f0;
            margin: 0;
            padding: 20px;
            font-size: 18px; /* Increased base font size */
        }

        /* Top header with navy blue to rich purple blend (top to bottom) */
        .gradient-header {
            background: linear-gradient(to bottom, #000080, #800080); /* Navy Blue to Rich Purple */
            color: white;
            padding: 15px;
            margin-bottom: 10px;
            border-radius: 10px;
            text-align: center;
        }
        
        /* Section headings with maroon to bright orange gradient (left to right) */
        .section-heading {
            background: linear-gradient(to right, #FF00FF, #FFA500); /* Magenta to Forest Green */
            color: white;
            padding: 10px;
            margin-bottom: 5px; /* Reduced margin to close gap */
            border-radius: 10px;
        }
        
        /* Subsection headings with magenta to forest green gradient (top to bottom) */
        .subsection-heading {
            background: linear-gradient(to right, #800000, #FFA500); /* Maroon to Bright Orange */
            color: white;
            padding: 8px;
            margin-bottom: 5px; /* Reduced margin */
            border-radius: 10px;
        }


        h1 {
            font-size: 32px; /* Increased font size */
            margin: 0;
        }

        h2 {
            font-size: 26px; /* Increased font size */
            margin-bottom: 5px; /* Reduced margin to close gap */
        }

        h3 {
            font-size: 22px; /* Increased font size */
        }

        p {
            color: #333;
            line-height: 1.7; /* Improved line spacing for readability */
            font-size: 20px; /* Increased paragraph font size */
        }

        img {
            display: block;
            max-width: 100%;
            height: auto;
            margin: 0 auto 10px auto; /* Center images horizontally */
            border: 2px solid #ddd;
            border-radius: 10px;
        }
        
        .content {
            padding: 5px 0; /* Reduced padding */
        }
    </style>
</head>
<body>

    <div class="gradient-header">
        <h1>CS 180 Final Project Part 1 - Light Field Camera</h1>
    </div>

    <div>
        <h2 class="section-heading">Overview</h2>
        <div class="content">
          <p>In this project, I recreated the effects of light field cameras using techniques like depth refocusing and aperture adjustment, inspired by the paper 
            <em>Light Field Photography with a Hand-held Plenoptic Camera</em> by Ren Ng et al. (Ren Ng, founder of the Lytro camera, is also a professor at 
            Berkeley!). By capturing multiple images over a plane orthogonal to the optical axis, complex effects such as dynamic focus and aperture control 
            can be achieved through simple operations like shifting and averaging. </p>
          <p> Using the Stanford Light Field Archive dataset, which contains images captured from a 17x17 
            camera array, I simulated varying focal lengths and aperture sizes. Each image corresponds 
            to a unique camera position, enabling the exploration of these light field techniques with 
            real-world data. </p>
          <p> My goal was to reproduce the fascinating visual effects described in Section 4 of the paper. 
            By leveraging the rectified image datasets provided by the Stanford Light Field Archive, I 
            implemented depth refocusing and aperture adjustment to demonstrate how elementary operations 
            on light field data can produce visually stunning results. </p>
            
        </div>
    </div>

    <div>
        <h2 class="section-heading">Depth Refocusing</h2>
        <div class="content">
            <p> Depth refocusing leverages the principle that objects farther from the camera exhibit 
              minimal positional variation across images in a light field, while closer objects shift 
              significantly when the camera moves. By appropriately shifting and averaging images 
              from a 17x17 camera grid, we can align objects at specific depths. This effectively simulates
              a change in focus. </p>
            <p> In this project, I used the center image at position (8, 8) as my reference and 
              shifted each image in the grid by \(\alpha \cdot (i - 8, j - 8)\), where \(\alpha\) is 
              a variable parameter that controls the focal depth. Averaging the shifted images 
              allows for dynamically focusing on objects at different distances. Without any shifting, 
              the final image appears sharp for far-away objects, but blurry for closer objeccts. </p>
        </div>

      <h3 class="subsection-heading">Results</h3>
        <div class="content">
          <p> This technique produces effects similar to adjusting the depth of focus in the 
            dataset's online viewer under the "Full Aperture" setting. Below are examples demonstrating 
            how changing the parameter \(\alpha\) creates images focused at varying depths.</p>
            <img src="toy_pan.png" alt="Toy Panorama">
        </div>
      
    </div>

    <div>
        <h2 class="section-heading">Aperture Adjustment</h2>
        <div class="content">
            <p>Aperture adjustment simulates the effect of varying aperture sizes in a light field camera 
               by controlling the number of images used in the averaging process. Averaging a large 
              number of images from the camera grid mimics a camera with a large aperture, capturing 
              more light and resulting in a shallower depth of field. Conversely, using fewer 
              images corresponds to a smaller aperture, with a greater depth of field.</p>
          <p> To achieve this, I introduced a hyperparameter \(\beta\) to control the radius of 
            the camera grid. Only images satisfying \(|i - 8| \leq \beta\) and \(|j - 8| \leq \beta\) were 
            included in the shifting and averaging process. By varying \(\beta\), I generated images 
            corresponding to different aperture sizes, all while maintaining focus on the same 
            depth. Larger \(\beta\) values produce a blurred background with focused objects, while 
            smaller \(\beta\) values retain sharpness across the scene. </p>

      <h3 class="subsection-heading">Results</h3>
        <div class="content">
          <p> Below are examples illustrating how changing \(\beta\) simulates varying 
            aperture sizes and their impact on the depth of field. </p>
        </div>
          
    </div>

          <div>
        <h2 class="section-heading">Summary</h2>
        <div class="content">
           <p>This project was an exciting exploration of light fields and their applications in 
             computational photography. I learned how simple operations like shifting and 
             averaging across a camera grid can produce complex effects like depth refocusing and 
             aperture adjustment. It was fascinating to see how different aspects of a camera, 
             such as aperture size and focal depth, contribute to the overall visual effects in an image. </p>
        </div>

    <div class="gradient-header">
        <h1>Project Part 2 - Eulerian Video Magnification</h1>
    </div>
      
    <div>
        <h2 class="section-heading">Detecting Corner Features</h2>
        <div class="content">
           <p> In Part A, I manually selected correspondence points to compute the homography matrix. Later, I automated this process by implementing the Harris Corner Detector, 
               which identifies interest points, or corners, within an image. A corner is a point where two edges meet, creating a significant change in brightness, and Harris corners 
               are reliable for feature detection as they are invariant to translation, rotation, and illumination changes.</p>
            <p> The detector calculates the structure tensor \( M \) using image derivatives in the x and y directions. Each window is scored using \( R = \det(M) - k(\text{trace}(M))^2 \),
                where the eigenvalues of \( M \) determine if the region contains a corner, edge, or flat area. I applied the Harris detector to both the left and right images to extract 
                key corner features automatically.</p>
        </div>

        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="harris_toy.png" alt="Toy Panorama">
        </div>

        <h3 class="subsection-heading">Adaptive Non-Maximal Suppression (ANMS)</h3>
        <div class="content">
            <p>While the Harris Corner Detector identifies many potential feature points, it often produces too many. To address this, Adaptive Non-Maximal Suppression (ANMS) filters the 
                points by distributing them evenly across the image, keeping only the strongest features. ANMS calculates a suppression radius for each point defined as the smallest 
                distance to a significantly stronger point using a robustness parameter \(c = 0.9\).</p> 
            <p>The algorithm works by computing the Euclidean distance and strength differences between each point and others. Points with non-positive strength differences are 
                discarded. The nearest valid distance is stored in a min-heap, which maintains the largest suppression radii. While ANMS may not return the points with the highest 
                strengths, it ensures that the selected features are spatially well-distributed, providing strong, dominant points across the image. The final set of Harris points is 
                shown below.</p>
            <img src="anms_toy.png" alt="Toy Panorama">
        </div>
    </div>

    <div>
        <h2 class="section-heading">Extracting Feature Descriptors</h2>
        <div class="content">
            <p>With a reduced set of Harris points, the next step is to match points between images using feature descriptors. Each descriptor is created by extracting a 40x40 window 
                centered on a Harris point. This patch captures a 20-pixel radius around the point in all directions. To generate a smooth, blurred descriptor, the patch is downsampled to 
                8x8 pixels instead of extracting a smaller patch directly.</p> 
            <p>The descriptors are then normalized to have a mean of 0 and a standard deviation of 1, making them robust to affine changes in intensity (e.g. bias and gain). These 
                normalized feature descriptors allow consistent and reliable matching between images.</p>
        </div>
    </div>

    <div>
        <h2 class="section-heading">Matching Feature Descriptors</h2>
        <div class="content">
            <p>To match feature descriptors, we compare all descriptor pairs between the two images using Sum of Squared Differences (SSD) or Nearest-Neighbor (NN) scores, where lower 
                scores indicate better matches. We first narrow the candidates by selecting each feature’s nearest neighbor. Next, we calculate the ratio between the 
                nearest-neighbor score and the second-nearest-neighbor score. If this ratio is below a chosen threshold (0.4 worked well in my testing), we accept the match. The resulting 
                matches are visualized with randomly colored points to highlight correspondences between the images.</p>
        </div>
        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <p> Even though some mismatches remain, the majority of the matches are accurate and reliable.</p>
            <img src="matches.png" alt="Toy Panorama">
        </div>
    </div>

    <div>
        <h2 class="section-heading">RANSAC</h2>
        <div class="content">
            <p>To eliminate false positives from the feature matching process, we apply RANSAC. In each iteration, a random set of 4 matches is selected to compute a homography. We then 
                evaluate how well this homography aligns the remaining matches, counting the number of inliers it produces. The homography with the highest number of inliers is selected 
                as the final result. While some true matches may be lost, most correct matches are retained. Also, all incorrect matches are removed which leads to more reliable stitching.</p>
        </div>
        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="ransac.png" alt="Toy Panorama">
        </div>
    </div>

     <div>
        <h2 class="section-heading">Blending into a Mosaic</h2>
        <div class="content">
            <p>I used the same techniques as Part A. First, I computed a homography matrix to warp one image onto the plane of another. Then, I calculated the bounding box and necessary shifts to align the 
              warped image with the target image. Finally, both images were stitched together within the mosaic by placing them at their computed positions in the combined frame.</p>
            <p>I applied this process to create a mosaic of stuffed animals (real-world example) and used blending techniques to eliminate sharp edges. I also experimented with Dota 2 
              screenshots to explore how parts of a video game map look on a larger scale, as the in-game view is limited. I tested the process on different landscapes, including the 
              “lane,” which contains mostly similar textures, and the “trees,” which has intricate leaf details.</p>
        </div>

        <h3 class="subsection-heading">Real-Life Results</h3>
        <div class="content">
            <p>Below are my manually and automatically stitched results side by side. They both appear to be similar.</p>
            <img src="compare_toy.png" alt="Toy Panorama">
        </div>

        
        <h3 class="subsection-heading">Game Results (Lane)</h3>
        <div class="content">
            <p>Below are my manually and automatically stitched results side by side. They both appear to be similar.</p>
             <img src="comparee_lane.png" alt="Dota">
        </div>

        <h3 class="subsection-heading">Game Results (Trees)</h3>
        <div class="content">
            <p>Below are my manually and automatically stitched results side by side. The automatic stitching noticeably outperformed the manual approach, as evidenced by the absence of a shadow artifact along the 
                bottom edge in the automatically stitched image (around 650 on the horizontal axis). This comparison highlights the precision and effectiveness of the automated process.</p>
            <img src="compare_creep.png" alt="Tree">
        </div>
    </div>

        <div>
        <h2 class="section-heading">What I learned</h2>
        <div class="content">
            <p> The most mind-blowing part of this project has been discovering the power of feature matching based solely on pixel intensities. I had always assumed that recognizing 
                similar points across images required a neural network or some form of “smart” AI to make sense of it all. But seeing how pixel intensities alone can accurately pinpoint 
                matching features between images was eye-opening! It felt like unlocking a new layer of understanding in computer vision—realizing that the simplest data can sometimes 
                yield the most precise results. </p>
        </div>
    </div>

            
</body>
</html>
