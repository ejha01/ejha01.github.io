<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>CS 180 Project 5 - Fun With Diffusion Models! by Eshani Jha</title>
    <style>
        body {
            font-family: 'Cambria', serif; /* Cambria font */
            background-color: #f0f0f0;
            margin: 0;
            padding: 20px;
            font-size: 18px; /* Increased base font size */
        }

        /* Top header with navy blue to rich purple blend (top to bottom) */
        .gradient-header {
            background: linear-gradient(to bottom, #000080, #800080); /* Navy Blue to Rich Purple */
            color: white;
            padding: 15px;
            margin-bottom: 10px;
            border-radius: 10px;
            text-align: center;
        }
        
        /* Section headings with maroon to bright orange gradient (left to right) */
        .section-heading {
            background: linear-gradient(to right, #FF00FF, #FFA500); /* Magenta to Forest Green */
            color: white;
            padding: 10px;
            margin-bottom: 5px; /* Reduced margin to close gap */
            border-radius: 10px;
        }
        
        /* Subsection headings with magenta to forest green gradient (top to bottom) */
        .subsection-heading {
            background: linear-gradient(to right, #800000, #FFA500); /* Maroon to Bright Orange */
            color: white;
            padding: 8px;
            margin-bottom: 5px; /* Reduced margin */
            border-radius: 10px;
        }


        h1 {
            font-size: 32px; /* Increased font size */
            margin: 0;
        }

        h2 {
            font-size: 26px; /* Increased font size */
            margin-bottom: 5px; /* Reduced margin to close gap */
        }

        h3 {
            font-size: 22px; /* Increased font size */
        }

        p {
            color: #333;
            line-height: 1.7; /* Improved line spacing for readability */
            font-size: 20px; /* Increased paragraph font size */
        }

        img {
            display: block;
            max-width: 100%;
            height: auto;
            margin: 0 auto 10px auto; /* Center images horizontally */
            border: 2px solid #ddd;
            border-radius: 10px;
        }
        
        .content {
            padding: 5px 0; /* Reduced padding */
        }
    </style>
</head>
<body>

    <div class="gradient-header">
        <h1>CS 180 Project 5 - Fun With Diffusion Models! by Eshani Jha</h1>
    </div>

    <div>
        <h2 class="section-heading">Overview</h2>
        <div class="content">
            <p>In this project, I will implement and deploy diffusion models for image generation, exploring their potential for creating high-quality, realistic visuals. The project is divided 
                into two distinct parts, each with its own set of goals and deadlines (detailed below). This step-by-step approach will ensure a comprehensive understanding of diffusion-based 
                generative modeling techniques.</p>
        </div>        
    </div>

    <div>
        <h2 class="section-heading">Setup</h2>
        <div class="content">
            <p> Using DeepFloyd's Stage 1 function, I generated images from text prompts, followed by applying Stage 2 for super-resolution to enhance the output quality. My 
                selected seed was <code>180</code>, ensuring reproducibility across all experiments. The prompts I used were: 
            </p>
            <ul>
                <li>"An oil painting of a snowy mountain village"</li>
                <li>"A man wearing a hat"</li>
                <li>"A rocket ship"</li>
            </ul>
            <p> To evaluate the effect of inference steps on image quality, I tested with 5, 20, and 50 steps. Unsurprisingly, 50 inference steps 
                produced the best results, showcasing fine details and higher fidelity to the prompts. Among the outputs, the "rocket ship" prompt was the most creative, 
                generating imaginative and visually striking results. While the model generally aligns well with the prompts, occasional discrepancies highlight its 
                limitations, particularly for simpler or abstract concepts. </p>
        </div>

        <div class="content">
            <img src="sec1_a.jpg" alt="Image 1" style="display: block; margin: 10px auto;">
            <img src="sec1_b.jpg" alt="Image 2" style="display: block; margin: 10px auto;">
            <img src="sec1_c.jpg" alt="Image 3" style="display: block; margin: 10px auto;">
    </div>
        
    </div>

    <div>
        <h2 class="section-heading">1.1 Implementing the Forward Process</h2>
        <div class="content">
        <p>To implement the forward process, we created a function to add noise to an image while adhering to DeepFloyd’s pre-trained model constraints: a non-linear noise 
            schedule (defined by <code>alphas_cumprod</code>) and 1000 discrete training steps (t-values ranging from 0 to 999). Below are the results of the forward function at various noise levels 
            (<code>t = 250</code>, <code>t = 500</code>, <code>t = 750</code>).</p>
        </div>
        
        <div class="content">
            <img src="sec11_a.jpg" alt="Points">
        </div>
    </div>

    <div>
        <h2 class="section-heading">1.2 Classical Denonising</h2>
        <div class="content">
            <p>Before turning to advanced methods, we first explored classical denoising with Gaussian blur filters to remove noise from the images at timesteps 
                <code>t = 250</code>, <code>t = 500</code>, and <code>t = 750</code>. While Gaussian filtering can reduce noise, it inevitably blurs finer details, making 
                it insufficient for restoring the original image quality. By manually tuning kernel sizes and sigma values, we attempted to strike a balance between noise 
                reduction and detail preservation, but the results were limited.</p>
            <p>To counteract the loss of detail, we applied sharpening filters (as explored in Project 2) by subtracting one Gaussian level from another and 
                adding the sharpness back to the original image. However, this could not recover lost information, underscoring the limitations of classical approaches in 
                handling such noise. Creative digital techniques will be explored further in the next section.</p>
        </div>
        <div class="content">
            <img src="sec12_a.png" alt="noise">
    </div>

    <div>
        <h2 class="section-heading">1.3 Diffusion One-Step Denoising</h2>
        <div class="content">
            <p>In this section, we used a pretrained diffusion model to perform one-step denoising, leveraging a UNet trained on a massive dataset of image pairs \((x_0, x_t)\). 
                The model estimates the noise in a noisy image conditioned on a text prompt embedding (e.g., "a high quality photo") and timestep \(t\). By scaling and removing 
                the predicted noise, we can approximate the original clean image.</p>
            <p>Results for timesteps  <code>t = 250</code>, <code>t = 500</code>, and <code>t = 750</code> show that the model performs well for low-noise images but struggles with higher noise 
                levels, resulting in blurrier outputs. While single-shot denoising is effective, its limitations highlight the need for iterative noise reduction, which we will 
                explore in the next section.</p>
        </div>

        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="sec13_a.png" alt="Image 1" style="display: block; margin: 10px auto;">
            <img src="sec13_b.png" alt="Image 2" style="display: block; margin: 10px auto;">
            <img src="sec13_c.png" alt="Image 3" style="display: block; margin: 10px auto;">
        </div>
    </div>

    <div>
        <h2 class="section-heading">1.4 Iterative Diffusion Denoising</h2>
        <div class="content">
            <p>In this section, I implemented iterative diffusion denoising, breaking the process into multiple steps to progressively refine the image. Unlike the single-step 
                approach, iterative denoising encourages the model to focus on coarse-to-fine details, restoring more intricate features over time. For example, the foliage in 
                the background of the Campanile image showed significantly improved definition compared to other methods.</p>
            <p>I compared three approaches: iterative diffusion, single-step diffusion, and Gaussian blur denoising. The iterative approach consistently outperformed the others, 
                producing sharper and more detailed results. Given that noise removes information, the iterative process “hallucinates” lost details, sometimes adding creative 
                interpretations to the image. The results demonstrate the power of diffusion models when combined with incremental refinement.</p>
        </div>

        <h3 class="subsection-heading">Real-Life Results</h3>
        <div class="content">
            <img src="sec14_a.png" alt="Toy Panorama">
        </div>


    <div>
        <h2 class="section-heading">1.5 Diffusion Model Sampling</h2>
        <div class="content">
           <p>In this section, I used the diffusion model to generate images from pure noise, effectively “hallucinating” visuals from scratch. By setting <code>i_start = 0</code> 
               and initializing with random noise, the iterative denoising process refined the noise into recognizable images. The results demonstrate how diffusion models can 
               synthesize images purely from the latent manifold of the dataset.</p>
            <p>I observed that lower strides (more iterative steps) produced richer, more detailed images, while higher strides often resulted in incomplete outputs, such as solid 
                black or purple images. All generated samples were enhanced using super-resolution. Below are five examples of "a high quality photo" created through this process.</p>
        </div>

        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="sec15_a.png" alt="Toy Panorama">
        </div>

        <h3 class="section-heading">1.6 Classifier-Free Guidance (CFG)</h3>
        <div class="content">
            <p>To enhance the controllability and quality of image generation, I implemented Classifier-Free Guidance (CFG). This technique allows us to adjust the influence of the 
                text prompt on the generated image by blending conditional and unconditional noise estimates. By scaling the difference between these estimates with a CFG scale, 
                we can balance fidelity to the prompt against image diversity.</p>
            <p>The most exciting aspect of CFG is the ability to push beyond the dataset's original manifold by using high CFG values, producing creative and otherworldly results. 
                For example, at a CFG scale of 7, the generated images take on surreal and imaginative qualities. Below are five examples generated with CFG scales of 2 and 7, 
                demonstrating the enhanced control and creativity unlocked by this approach.</p>
        
        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="sec16_a.png" alt="Toy Panorama">
        </div>

    </div>

    <div>
        <h2 class="section-heading">1.7 Image-to-Image Translation</h2>
        <div class="content">
            <p>In this section, I applied the SDEdit algorithm to perform image-to-image translation, using the denoising process of diffusion models to make creative edits to 
                existing images. Starting with a noisy version of the test image, I iteratively denoised it at different noise levels (<code>i_start = [1, 3, 5, 7, 10, 20]</code>) 
                with the text prompt "a high quality photo." The results show progressively more refined edits, with lower noise levels retaining more original features and 
                higher noise levels producing larger transformations.</p>
            <p>In addition to the test image, I experimented with two of my own images: a beach scene and a skyscraper. Using the same procedure, I observed similar patterns of 
                transformation, where the model gradually blended creative details into the original images. The flexibility of this approach highlights the potential of diffusion 
                models for controlled and imaginative edits.</p>
        </div>
    </div>

        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="sec17_a.png" alt="Image 1" style="display: block; margin: 10px auto;">
            <img src="sec17_b.png" alt="Image 2" style="display: block; margin: 10px auto;">
            <img src="sec17_c.png" alt="Image 3" style="display: block; margin: 10px auto;">
        </div>
    </div>

    <div>
        <h2 class="section-heading">1.7.1 Editing Hand-Drawn and Web Images</h2>
        <div class="content">
            <p>This section explores how diffusion models can project non-realistic images, such as hand-drawn sketches or web-sourced illustrations, onto the natural image 
                manifold. Using the same iterative denoising process as before, I experimented with a downloaded image of an octopus and two hand-drawn sketches: a flower and a 
                house. Noise levels of <code>i_start = [1, 3, 5, 7, 10, 20]</code> were applied to observe the gradual transformation from noisy inputs to refined outputs.</p>
            <p>For the octopus image, the model successfully enhanced realism with each step, adding textures and lifelike details. The hand-drawn flower and house images 
                showed varying levels of success, with the flower achieving a more natural appearance, while the house introduced creative interpretations. These results 
                highlight the versatility of diffusion models in transforming non-realistic inputs into more polished outputs.</p>
        </div>
        
        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="sec171_a.jpg" alt="Image 1" style="display: block; margin: 10px auto;">
            <img src="sec171_b.jpg" alt="Image 2" style="display: block; margin: 10px auto;">
            <img src="sec171_c.jpg" alt="Image 3" style="display: block; margin: 10px auto;">
        </div>
    </div>
    </div>

    <div>
        <h2 class="section-heading">1.7.2 Inpainting</h2>
        <div class="content">
                <p>In this section, I explored the powerful inpainting capabilities of diffusion models, where missing or masked areas of an image are seamlessly filled based on 
                    the surrounding context. Using a mask, the model selectively preserves unmasked areas while iteratively hallucinating new content in the masked regions, 
                    effectively integrating them into the original image.</p>
                <p>I applied inpainting to three images: the Campanile, a beach scene, and the Manhattan skyline. For the Campanile, I masked the top portion of the tower, and 
                    the model successfully reconstructed it while blending it naturally with the existing structure. In the beach and Manhattan skyline images, the inpainting 
                    process creatively restored missing areas, producing results that aligned convincingly with the original content. These experiments highlight the 
                    versatility and robustness of diffusion-based inpainting.</p>
        </div>
        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="sec172_a.jpg" alt="Image 1" style="display: block; margin: 10px auto;">
            <img src="sec172_b.png" alt="Image 2" style="display: block; margin: 10px auto;">
            <img src="sec172_c.png" alt="Image 3" style="display: block; margin: 10px auto;">
            <img src="sec172_d.png" alt="Image 4" style="display: block; margin: 10px auto;">
        </div>
    </div>

     <div>
        <h2 class="section-heading">1.7.3 Text-Conditional Image-to-Image Translation</h2>
        <div class="content">
            <p>In this section, I explored text-conditional image-to-image translation, combining the structure of an input image with the creativity of a text prompt. 
                By varying noise levels (<code>i_start = [1, 3, 5, 7, 10, 20]</code>), the model interpolates between the input image and the text prompt, blending features 
                from both. Higher noise levels allow the model to move closer to the text prompt's "location" in the latent space, while lower noise levels retain more of the 
                original image.</p>
            <p>I experimented with three combinations: the Campanile paired with the prompt "a rocket ship," a beach scene paired with "a pencil," and the Manhattan skyline 
                paired with "a photo of a dog." The results show creative transformations, with the Campanile gradually morphing into a futuristic rocket ship, the beach taking on artistic 
                pencil-like textures, and the skyline blending cat-like features into its structure. These experiments highlight the versatility and control offered by 
                text-conditional diffusion models.</p>
        </div>

        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="sec173_a.jpg" alt="Image 1" style="display: block; margin: 10px auto;">
            <img src="sec173_b.jpg" alt="Image 2" style="display: block; margin: 10px auto;">
            <img src="sec173_c.jpg" alt="Image 3" style="display: block; margin: 10px auto;">
        </div>
    </div>

    <div>
        <h2 class="section-heading">1.8 Visual Anagrams</h2>
        <div class="content">
            <p>In this section, I created visual anagrams—images that appear as one subject when viewed normally and transform into another subject when flipped upside down. 
                Using diffusion models, I combined two prompts by denoising the image with one prompt, flipping the image, denoising it with the second prompt, 
                and averaging the two noise estimates. This process generates a single image that reveals two distinct interpretations based on orientation.</p>
            <p>I experimented with three visual anagrams: 
            <ul>
                <li><strong>Prompt 1:</strong> "An oil painting of an old man" and <strong>Prompt 2:</strong> "An oil painting of people around a campfire"</li>
                <li><strong>Prompt 1:</strong> "A photo of a hipster barista" and <strong>Prompt 2:</strong> "A rocket ship"</li>
                <li><strong>Prompt 1:</strong> "An oil painting of a snowy mountain village" and <strong>Prompt 2:</strong> "A lithograph of a skull"</li>
            </ul>
            The results were striking, with each image seamlessly transitioning between the two prompts when flipped, showcasing the creative potential of diffusion models 
            for optical illusions.
        </p>
        </div>

        <h3 class="subsection-heading">Results</h3>
        <div class="content" style="display: flex; flex-direction: column; gap: 20px;">
    <!-- Row 1 -->
    <div style="display: flex; justify-content: center; gap: 20px;">
        <img src="sec18_a1.png" alt="Image 1 Row 1" style="width: 20%;">
        <img src="sec18_a2.png" alt="Image 2 Row 1" style="width: 20%;">
    </div>
    <!-- Row 2 -->
    <div style="display: flex; justify-content: center; gap: 20px;">
        <img src="sec18_b1.png" alt="Image 1 Row 2" style="width: 20%;">
        <img src="sec18_b2.png" alt="Image 2 Row 2" style="width: 20%;">
    </div>
    <!-- Row 3 -->
    <div style="display: flex; justify-content: center; gap: 20px;">
        <img src="sec18_c1.png" alt="Image 1 Row 3" style="width: 20%;">
        <img src="sec18_c2.png" alt="Image 2 Row 3" style="width: 20%;">
    </div>
</div>

    </div>

     <div>
        <h2 class="section-heading">1.9 Hybrid Images</h2>
        <div class="content">
            <p>In this section, I implemented Factorized Diffusion to create hybrid images that combine two distinct subjects by blending their frequency components. The 
                process involves generating two noise estimates using different text prompts, applying low-pass and high-pass filters to separate their frequency components, 
                and combining the low frequencies from one with the high frequencies from the other. This technique results in images that appear as one subject from afar but 
                transform into another up close.</p>
            <p> I created three hybrid images using this method:
                    <ul>
                        <li><strong>Hybrid 1:</strong> "A lithograph of a skull" (low frequencies) and "A lithograph of waterfalls" (high frequencies)</li>
                        <li><strong>Hybrid 2:</strong> "A lithograph of waterfalls" (low frequencies) and "An oil painting of people around a campfire" (high frequencies)</li>
                        <li><strong>Hybrid 3:</strong> "A lithograph of a skull" (low frequencies) and "A photo of a man" (high frequencies)</li>
            </ul>
            The results showcase seamless blending, with each hybrid image transitioning between two distinct interpretations depending on viewing distance. This experiment 
            highlights the versatility of diffusion models in creating visually compelling and conceptually rich outputs.
            </p>
        </div>

        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="sec_19a.jpg" alt="Image 1" style="display: block; margin: 10px auto;">
            <img src="sec_19b.jpg" alt="Image 2" style="display: block; margin: 10px auto;">
            <img src="sec_19c.jpg" alt="Image 3" style="display: block; margin: 10px auto;">
        </div>
    </div>
    </div>


        <div>
        <h2 class="section-heading">5B - Part 1: Training a Single-Step Denoising U-Net</h2>
        <p>
The first step in this project involved building a simple one-step denoiser. The objective is to train a UNet-based model to map a noisy image \(x_t\) to its clean counterpart \(x_0\) by minimizing an L2 loss:
</p>

<p>
The UNet architecture used here features downsampling and upsampling blocks with skip connections to retain spatial details during the denoising process. The primary operations include:
</p>
<ul>
    <li><strong>Conv2d:</strong> Convolutional layers for adjusting channel dimensions without altering spatial resolution.</li>
    <li><strong>DownConv:</strong> Convolutional layers that reduce tensor dimensions by a factor of 2.</li>
    <li><strong>UpConv:</strong> Transpose convolutional layers that upsample tensor dimensions by a factor of 2.</li>
    <li><strong>Flatten:</strong> Average pooling layers that compress tensors into 1x1 dimensions.</li>
    <li><strong>Concat:</strong> Channel-wise concatenation using skip connections to merge information across resolutions.</li>
</ul>
<p>
To deepen the network, composed operations like <em>ConvBlock</em>, <em>DownBlock</em>, and <em>UpBlock</em> were used. These include additional learnable parameters for enhanced performance while maintaining input-output shape consistency. This UNet serves as the foundation for our single-step denoising tasks.
</p>
        <h3 class="subsection-heading">Results</h3>
        <div class="content">
            <img src="sec172_a.jpg" alt="Image 1" style="display: block; margin: 10px auto;">
        </div>
    </div>


        <div>
        <h2 class="section-heading">1.2 Using the UNet to Train a Denoiser</h2>
<p>
The objective was to train the UNet denoiser to restore clean images from noisy inputs, using the MNIST dataset. Each image batch was noised dynamically during training, ensuring the network saw new variations with every epoch for better generalization. Key training details include:
</p>
<ul>
    <li><strong>Dataset:</strong> MNIST </li>
    <li><strong>Batch Size:</strong> 256</li>
    <li><strong>Epochs:</strong> 5</li>
    <li><strong>Model:</strong> UNet with hidden dimension \(D = 128\)</li>
    <li><strong>Optimizer:</strong> Adam with learning rate \(1 \times 10^{-4}\)</li>
</ul>
<p>
I visualized the training process with a loss curve and evaluated denoised outputs after the 1st and 5th epochs. As expected, the results showed significant improvements after 5 epochs, with cleaner reconstructions of the noisy test digits.
</p>

<h3>1.2.2 Out-of-Distribution Testing</h3>
<p>
To test the denoiser's robustness, I evaluated its performance on out-of-distribution noise levels, varying the noise parameter \(\sigma\) beyond the range seen during training. The results revealed the model's limitations with high noise levels, as the reconstructions became increasingly distorted, while moderate noise levels still produced recognizable digits.
</p>
            
</body>
</html>
